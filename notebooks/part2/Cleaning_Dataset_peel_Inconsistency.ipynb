{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_Dataset_peel_Inconsistency.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ****IMPORTANT NOTE:**\n",
        "\n",
        "This notebook is for VIEW ONLY. To test and run the notebook, please download, upload and run the zeppelin notebook on Peel: [Cleaning_Dataset_peel_Inconsistency.zpln](https://github.com/qyc206/evq_big_data_project/blob/main/notebooks/part2/Cleaning_Dataset_peel_Inconsistency.zpln)."
      ],
      "metadata": {
        "id": "5XgciGtmWIZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the dataset to Peel cluster & Define dataset path\n",
        "\n",
        "Before continuing, make sure your dataset is available on Peel HDFS. If your dataset is on your local machine, you can copy them to the login node of the cluster and move them to your user directory in the HDFS using the following commands:\n",
        "\n",
        "```\n",
        "# Copy file from local machine to login node of the cluster\n",
        "mylaptop$ scp -r [FILENAME] <net_id>@peel.hpc.nyu.edu:~/\n",
        "\n",
        "# Move file from cluster login node to your user directory in HDFS \n",
        "# (your file will be in the path \"/user/[netid]/[FILENAME]\")\n",
        "hfs -put [FILENAME]\n",
        "```\n",
        "\n",
        "Make sure you can locate your dataset before continuing onwards."
      ],
      "metadata": {
        "id": "CIq7CvhvWoGO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJGjxHmwWEjk"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Define path to dataset on Peel HDFS (NOTE: replace file name with your own if different)\n",
        "dataset_path = \"/user/CS-GY-6513/project_data/data-cityofnewyork-us.erm2-nwe9.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Spark Session\n",
        "\n",
        "Now that the dataset is uploaded and the path is defined, we need to set up pyspark to begin profiling and exploring our dataset. \n",
        "\n",
        "If this notebook is run in an environment where pyspark is not yet installed, please add a new cell BEFORE the next cell and run the following command:\n",
        "\n",
        "```\n",
        "# Run this command if pyspark is not already installed\n",
        "%pip install pyspark\n",
        "```"
      ],
      "metadata": {
        "id": "dqQbc_yMWs_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Set up pyspark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .appName(\"Python Spark SQL basic example\") \\\n",
        "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "            .config(\"spark.executor.memory\", \"50g\") \\\n",
        "            .config(\"spark.driver.memory\", \"50g\") \\\n",
        "            .getOrCreate()"
      ],
      "metadata": {
        "id": "0e2PTy0-WvLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset using spark\n",
        "\n",
        "Run the following lines to load the dataset using spark and test to make sure that dataset is properly loaded."
      ],
      "metadata": {
        "id": "Fp326TXaWzXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Load dataset\n",
        "df = spark.read.format('csv').options(header='true',inferschema='true').load(dataset_path)\n",
        "# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\n",
        "df.createOrReplaceTempView(\"311_service_report\") "
      ],
      "metadata": {
        "id": "YXYxDbFMW1NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Type casting to expected types\n",
        "df = df.withColumn(\"Unique Key\",df[\"Unique Key\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Due Date\",to_timestamp(df[\"Due Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Created Date\", to_timestamp(df[\"Created Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Closed Date\",to_timestamp(df[\"Closed Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Incident Zip\",df[\"Incident Zip\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"BBL\",df[\"BBL\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"X Coordinate (State Plane)\",df[\"X Coordinate (State Plane)\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Y Coordinate (State Plane)\",df[\"Y Coordinate (State Plane)\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Resolution Action Updated Date\",to_timestamp(df[\"Resolution Action Updated Date\"],\"MM/dd/yyyy hh:mm:ss a\"))"
      ],
      "metadata": {
        "id": "S54FoYKxW3aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Inconsistency\n",
        "\n",
        "As seen during profiling, there is incorrect data mostly in the \"Agency Name\" Column, but we will need to clean both \"Agency Name\" and \"Agency\" Columns.\n",
        "\n",
        "To go over changes in \"Agency\", we need to make specific changes. For example, there were formatting errors with \"Mayorâ\", and because both are specific cases, we will fix those manually. For the other errors that were shown in Agency, we can either filter out these errors, or we can check the \"correct\" dataset to see if the Agency Name has a match in the good dataset and change the Agency accordingly. However, our main focus will be on Agency Names because Agency Names has significantly more errors than Agency, and therefore, we will not be implementing these changes in this document at the moment.\n",
        "\n",
        "\"Agency Name\" will be fixed by going through if there is a corresponding \"Agency\" in the same row that is also in the \"correct\" dataset. If there is a match, then we will change the value of the agency name to what it is supposed to be. There will be one type of Agency I will not be changing the name for: the DOE. This is because of the aformentioned \"School\" agency name problem where the name of the school that the DOE could be important. The 3-1-1 names will also go untouched for the same reason."
      ],
      "metadata": {
        "id": "Imyn7lN2W7xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cells to fix “Mayorâ” problem."
      ],
      "metadata": {
        "id": "bIZEjh-WW-q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# Fix the Mayora Problem by finding \"MAYORâ\" and replacing it in Agency\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df = df.withColumn(\"Agency\", \\\n",
        "              when(df[\"Agency\"][0:6] == \"MAYORâ\", \"OSE\").otherwise(df[\"Agency\"]))"
      ],
      "metadata": {
        "id": "fOrXXA9gW8zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Fix the Mayora Problem by finding \"Mayorâ\" and replacing it in Agency Name\n",
        "\n",
        "df = df.withColumn(\"Agency Name\", \\\n",
        "              when(df[\"Agency Name\"][0:6] == \"Mayorâ\", \"Mayor's Special Office of Enforcement\").otherwise(df[\"Agency Name\"]))"
      ],
      "metadata": {
        "id": "AFa5eiNXXCJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on, make sure to download and upload the [NYC-Agency-Names.csv](https://drive.google.com/file/d/1EHpyXNOwCpv-NM0OTTqFBHeKikKpBtd1/view?usp=sharing) dataset into the same folder/directory as where our dataset resides (i.e. for this colab notebook environment, the current working directory path should be /content/drive/MyDrive/evq_311_service_proj and the datasets should all be placed in the dataset/ folder inside the current working directory).\n",
        "\n",
        "This dataset is needed for fixing inconsistencies in \"Agency Name\" and \"Agency\"."
      ],
      "metadata": {
        "id": "0_vRmDTNXFNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Define path for NYC Agency Names dataset\n",
        "# (Note: make sure to update to your netid and dataset name)\n",
        "nycAgencyNames_path = \"/user/emc689/NYC-Agency-Names.csv\""
      ],
      "metadata": {
        "id": "FtObqLgJXFh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Read NYC Agency Names dataset \n",
        "agency_df = spark.read.csv(nycAgencyNames_path, header=True)\n",
        "\n",
        "# Show Schema for NYC Agency Names dataset\n",
        "agency_df.printSchema()"
      ],
      "metadata": {
        "id": "fLE1Rra3XKtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cells to fix inconsistencies in “Agency Name” and “Agency”."
      ],
      "metadata": {
        "id": "2Qi6bRuUXNvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "from pyspark.sql.functions import coalesce\n",
        "\n",
        "# Checked the Agency Acronym and found the Agency Names for all rows this way any mistakes on spellings, uniformity can be removed at once.\n",
        "result = df.join(agency_df,df[\"Agency\"] ==  agency_df[\"Agency Acronym\"],\"left\").select(df[\"Unique Key\"],df[\"Agency\"],agency_df[\"Agency\"].alias(\"agency_name_df\"),df[\"Agency Name\"].alias(\"df_Agency\"))\n",
        "\n",
        "# We want to now turn the value of agency_name_df to null when df_Agency contains school\n",
        "result = result.withColumn(\"agency_name_df\", when( (result[\"Agency\"]== \"DOE\")& (result[\"df_Agency\"][0:6] == \"School\"), None).otherwise(result[\"agency_name_df\"]) )\n",
        "temp = result.select(result[\"Unique Key\"].alias(\"Temp Key\"),coalesce(result[\"agency_name_df\"], result[\"df_Agency\"]).alias(\"Final Agency\"))"
      ],
      "metadata": {
        "id": "iu2bnXvWXOCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# Update our Agency Name Column with the new Data\n",
        "df = df.join(temp,df[\"Unique Key\"] ==  temp[\"Temp Key\"],how=\"left\")\n",
        "df = df.drop(\"Temp Key\")\n",
        "df = df.withColumn(\"Agency Name\", df[\"Final Agency\"])\n",
        "df = df.drop(\"Final Agency\")"
      ],
      "metadata": {
        "id": "yMQrJOP7XPn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the following cells to see the improvements!\n"
      ],
      "metadata": {
        "id": "TAhgrITMXRK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# Show that the \"MAYORâ\" problem is fixed\n",
        "df.select(\"Agency\", \"Agency Name\").filter(df[\"Agency\"] == \"OSE\").show(20,False)"
      ],
      "metadata": {
        "id": "YnlgDSfuXTOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe in the following result that the Agency Name looks more consistent and more accurate."
      ],
      "metadata": {
        "id": "fPG7_YH7XU6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to further show the data set has improved by looking at incorrect data. We can observe that there is much less inconsistencies compared to what we saw during profiling (Note: we are not counting 3-1-1 and the variations of that as inconsistent)."
      ],
      "metadata": {
        "id": "6pirWY2tXWoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# All distinct agency name present in the dataset\n",
        "df5 = df.select(\"Agency Name\").distinct() \n",
        "list_of_agency_names = list(df5.toPandas()['Agency Name']) \n",
        "\n",
        "# All agency present in the dataset\n",
        "list_of_correct_agency_names = list(agency_df.select(\"Agency\").distinct().toPandas()['Agency']) "
      ],
      "metadata": {
        "id": "3NHlGQP-XWMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "from pyspark.sql.functions import asc\n",
        "\n",
        "# Show new values for Agency Name\n",
        "df.select(\"Agency Name\").distinct().orderBy(asc(\"Agency Name\")).show(df.count(), False)"
      ],
      "metadata": {
        "id": "Huw3nJGKXaoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "list_of_wrong_agency_names_final = []\n",
        "for i in list_of_agency_names:\n",
        "  if (i[0:6] != \"School\") and i not in list_of_correct_agency_names:\n",
        "    list_of_wrong_agency_names_final.append(i)\n",
        "for j in list_of_wrong_agency_names_final:\n",
        "  print(j)"
      ],
      "metadata": {
        "id": "6AO02ebHXdbN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}