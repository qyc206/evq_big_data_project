{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_Dataset_peel_Completeness.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ****IMPORTANT NOTE:**\n",
        "\n",
        "This notebook is for VIEW ONLY. To test and run the notebook, please download, upload and run the zeppelin notebook on Peel: [Cleaning_Dataset_peel_Completeness.zpln](https://github.com/qyc206/evq_big_data_project/blob/main/notebooks/part2/Cleaning_Dataset_peel_Completeness.zpln)."
      ],
      "metadata": {
        "id": "Ea9IWxNgckVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning\n",
        "\n",
        "This notebook goes over the cleaning process and results of cleaning the dataset based on the problems observed in the Profiling_The_Dataset.ipynb notebook. Some methods are further modified to have mroe general uses for datasets that are not the original. However, to show our algorithms, we use the original dataset as an example.\n",
        "\n",
        "Just like the notebook for profiling, this notebook will also be broken down into the following sections: Uniformity, Accuracy, Inconsistency, Completeness, and Outlier. Each section will show the solution that we have come up with to solve the problem posed in the corresponding section in the profiling notebook."
      ],
      "metadata": {
        "id": "g-WxHeH9cysy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the dataset to Peel cluster & Define dataset path\n",
        "\n",
        "Before continuing, make sure your dataset is available on Peel HDFS. If your dataset is on your local machine, you can copy them to the login node of the cluster and move them to your user directory in the HDFS using the following commands:\n",
        "\n",
        "```\n",
        "# Copy file from local machine to login node of the cluster\n",
        "mylaptop$ scp -r [FILENAME] <net_id>@peel.hpc.nyu.edu:~/\n",
        "\n",
        "# Move file from cluster login node to your user directory in HDFS \n",
        "# (your file will be in the path \"/user/[netid]/[FILENAME]\")\n",
        "hfs -put [FILENAME]\n",
        "```\n",
        "\n",
        "Make sure you can locate your dataset before continuing onwards."
      ],
      "metadata": {
        "id": "iFcXiQasc1ir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7Tygpe6chGQ"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Define path to dataset on Peel HDFS (NOTE: replace file name with your own if different)\n",
        "dataset_path = \"/user/CS-GY-6513/project_data/data-cityofnewyork-us.erm2-nwe9.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Spark Session\n",
        "\n",
        "Now that the dataset is uploaded and the path is defined, we need to set up pyspark to begin profiling and exploring our dataset. \n",
        "\n",
        "If this notebook is run in an environment where pyspark is not yet installed, please add a new cell BEFORE the next cell and run the following command:\n",
        "\n",
        "```\n",
        "# Run this command if pyspark is not already installed\n",
        "%pip install pyspark\n",
        "```"
      ],
      "metadata": {
        "id": "fAjEaAu9c4UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Set up pyspark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .appName(\"Python Spark SQL basic example\") \\\n",
        "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "            .config(\"spark.executor.memory\", \"50g\") \\\n",
        "            .config(\"spark.driver.memory\", \"50g\") \\\n",
        "            .getOrCreate()"
      ],
      "metadata": {
        "id": "hYD2Lsx6c6XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset using spark\n",
        "\n",
        "Run the following lines to load the dataset using spark and test to make sure that dataset is properly loaded."
      ],
      "metadata": {
        "id": "CCS6p8OKc8Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Load dataset\n",
        "df = spark.read.format('csv').options(header='true',inferschema='true').load(dataset_path)\n",
        "# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\n",
        "df.createOrReplaceTempView(\"311_service_report\") "
      ],
      "metadata": {
        "id": "HLIhO-WPc-Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the result of running the above cell that most items in the schema is of type string, even if it is not the expected type. To modify the dataset such that the types of each column is what we would expect, we perform type casting for each column that should not be type string.\n",
        "\n",
        "**NOTE: the following cell is specific for the 311 service report dataset; make sure to modify the following cell to include type casting that is necessary to your dataset, if different"
      ],
      "metadata": {
        "id": "eNM6vUUKdAHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Type casting to expected types\n",
        "df = df.withColumn(\"Unique Key\",df[\"Unique Key\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Due Date\",to_timestamp(df[\"Due Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Created Date\", to_timestamp(df[\"Created Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Closed Date\",to_timestamp(df[\"Closed Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Incident Zip\",df[\"Incident Zip\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"BBL\",df[\"BBL\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"X Coordinate (State Plane)\",df[\"X Coordinate (State Plane)\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Y Coordinate (State Plane)\",df[\"Y Coordinate (State Plane)\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Resolution Action Updated Date\",to_timestamp(df[\"Resolution Action Updated Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "\n",
        "\n",
        "# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\n",
        "df.createOrReplaceTempView(\"311_service_report\")\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "86SApEgMdBjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV. Completeness\n",
        "\n",
        "During profiling, we observed that several values in the City column are null (i.e. missing). One simple way to try and fix the city nulls is by using the \"Borough\" Column because the data set seems to use Borough names for city names. Therefore, we simply replace the null values that have a non null value for its Borough with the Borough name.\n",
        "\n",
        "Manhattan is a slightly special case because New York = Manhattan in the city column. For this case we have to change the value of \"MANHATTAN\" in Borough to New York.\n",
        "\n",
        "Any null value in \"City\" that also has a null \"Borough\" value will be marked as Unspecified."
      ],
      "metadata": {
        "id": "vkzR9qaWdD9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make this function more applicable to general uses, there a flag for whether you will be able to apply one column to complete the other, or if you are forced to only fill in “unspecified” for one column. If there are any two columns that have a similar relationship, the function could be tweaked (ie get rid of the Manhattan to New York cases) to fit the function better"
      ],
      "metadata": {
        "id": "-r5CZiN-dKg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# Replace null values with \"unspecified\"\n",
        "from pyspark.sql.functions import when, desc, initcap\n",
        "\n",
        "def completeColumn(datframe, targetCol,crossCheckCol,crossColFlag):\n",
        "    if(crossColFlag == True):\n",
        "        #if we want to cross check our data, we will cross check it here\n",
        "        datframe = datframe.withColumn(targetCol,\\\n",
        "              when(datframe[targetCol].isNull() & (datframe[crossCheckCol] == \"MANHATTAN\"), \"New York\").when(datframe[targetCol].isNull() &  datframe[crossCheckCol].isNotNull() & (datframe[crossCheckCol] != \"MANHHATAN\"), initcap(df[crossCheckCol])).when(datframe[targetCol].isNull() &  datframe[crossCheckCol].isNull(), \"Unspecified\").otherwise(datframe[targetCol]))\n",
        "    else:\n",
        "        #Mark the row as unspecified\n",
        "        datframe = datframe.withColumn(targetCol, when(datframe[targetCol].isNull(), \"Unspecified\").otherwi(datframe[targetCol]))\n",
        "    \n",
        "    return datframe"
      ],
      "metadata": {
        "id": "WJbVck8SdIqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Replace null values with \"unspecified\"\n",
        "\n",
        "df = completeColumn(df, \"City\",\"Borough\", True)"
      ],
      "metadata": {
        "id": "KEbsNMZYdOB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Re-display number of values in City column\n",
        "df.groupBy('City').count().orderBy(desc(\"count\")).show(20, False)"
      ],
      "metadata": {
        "id": "trl6ToZgdPdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# Count how many nulls left\n",
        "df.filter(df[\"City\"].isNull()).count()"
      ],
      "metadata": {
        "id": "mUaf8xl9dSrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}