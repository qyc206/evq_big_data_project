{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_Dataset_peel_Accuracy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ****IMPORTANT NOTE:**\n",
        "\n",
        "This notebook is for VIEW ONLY. To test and run the notebook, please download, upload and run the zeppelin notebook on Peel: [Cleaning_Dataset_peel_Accuracy.zpln](https://github.com/qyc206/evq_big_data_project/blob/main/notebooks/part2/Cleaning_Dataset_peel_Accuracy.zpln)."
      ],
      "metadata": {
        "id": "knDlLMDVoz3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Accuracy\n",
        "\n",
        "During profiling, we used the [uszips.csv](https://drive.google.com/file/d/1qd2cXgTx-h-hRd0C7z2s_U4O8VYLAXA7/view?usp=sharing) dataset to uncover several inaccuracies in the City column of our 311 service dataset. To solve this problem, we use the zipcodes in our dataset to match with those in the [uszips.csv](https://drive.google.com/file/d/1qd2cXgTx-h-hRd0C7z2s_U4O8VYLAXA7/view?usp=sharing) dataset so that we can find the actual city values to replace those that are inaccurate.\n",
        "\n",
        "Before we start, make sure to download and upload the [uszips.csv](https://drive.google.com/file/d/1qd2cXgTx-h-hRd0C7z2s_U4O8VYLAXA7/view?usp=sharing) dataset into HDFS just as described in the previous \"Upload the dataset to Peel cluster & Define dataset path\" section. After the reference dataset is downloaded and uploaded into HDFS, run the cells to define the dataset path and make sure the dataset can be read."
      ],
      "metadata": {
        "id": "4CMGBNdAu3LX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the dataset to Peel cluster & Define dataset path\n",
        "\n",
        "Before continuing, make sure your dataset is available on Peel HDFS. If your dataset is on your local machine, you can copy them to the login node of the cluster and move them to your user directory in the HDFS using the following commands:\n",
        "\n",
        "```\n",
        "# Copy file from local machine to login node of the cluster\n",
        "mylaptop$ scp -r [FILENAME] <net_id>@peel.hpc.nyu.edu:~/\n",
        "\n",
        "# Move file from cluster login node to your user directory in HDFS \n",
        "# (your file will be in the path \"/user/[netid]/[FILENAME]\")\n",
        "hfs -put [FILENAME]\n",
        "```\n",
        "\n",
        "Make sure you can locate your dataset before continuing onwards."
      ],
      "metadata": {
        "id": "NMGjAN-4u6K0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ5p0Evkowuo"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Define path to dataset on Peel HDFS (NOTE: replace file name with your own if different)\n",
        "dataset_path = \"/user/CS-GY-6513/project_data/data-cityofnewyork-us.erm2-nwe9.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Spark Session\n",
        "\n",
        "Now that the dataset is uploaded and the path is defined, we need to set up pyspark to begin profiling and exploring our dataset. \n",
        "\n",
        "If this notebook is run in an environment where pyspark is not yet installed, please add a new cell BEFORE the next cell and run the following command:\n",
        "\n",
        "```\n",
        "# Run this command if pyspark is not already installed\n",
        "%pip install pyspark\n",
        "```\n"
      ],
      "metadata": {
        "id": "_yH1-we3u-Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Set up pyspark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .appName(\"Python Spark SQL basic example\") \\\n",
        "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "            .config(\"spark.executor.memory\", \"35g\") \\\n",
        "            .config(\"spark.driver.memory\", \"35g\") \\\n",
        "            .getOrCreate()"
      ],
      "metadata": {
        "id": "UbdNuljevBBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset using spark\n",
        "\n",
        "Run the following lines to load the dataset using spark and test to make sure that dataset is properly loaded."
      ],
      "metadata": {
        "id": "Aneb49guvC5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Load dataset\n",
        "df = spark.read.format('csv').options(header='true',inferschema='true').load(dataset_path)\n",
        "# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\n",
        "df.createOrReplaceTempView(\"311_service_report\") "
      ],
      "metadata": {
        "id": "JHPp3qKTvFFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generalizing Formatting\n",
        "\n",
        "For many datasets, to optimally find information about any column that involves time, the column type must be turned into a timestamp type. However, to turn a column type into a timestamp, the data within the column must match the format that is specified when calling the to_timestamp() function ( to_timestamp(dataset[column], format) ). Therefore, it is best to be able to generalize this part of formating to make sure all our date columns are uniforom. This is even more essential since some of our solutions involve dates."
      ],
      "metadata": {
        "id": "31WyUhVsvMIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "def formatDate(dataset, col, DateForm):\n",
        "    formatedData = dataset.withColumn(col,to_timestamp(dataset[col],DateForm))\n",
        "    return formatedData"
      ],
      "metadata": {
        "id": "ExtO75ppvOqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Type casting to expected types\n",
        "df = df.withColumn(\"Unique Key\",df[\"Unique Key\"].cast(IntegerType()))\n",
        "df = formatDate(df,\"Due Date\",\"MM/dd/yyyy hh:mm:ss a\")\n",
        "df = formatDate(df,\"Created Date\",\"MM/dd/yyyy hh:mm:ss a\")\n",
        "df = formatDate(df,\"Closed Date\",\"MM/dd/yyyy hh:mm:ss a\")\n",
        "df = df.withColumn(\"Incident Zip\",df[\"Incident Zip\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"BBL\",df[\"BBL\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"X Coordinate (State Plane)\",df[\"X Coordinate (State Plane)\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Y Coordinate (State Plane)\",df[\"Y Coordinate (State Plane)\"].cast(IntegerType()))\n",
        "df = formatDate(df,\"Resolution Action Updated Date\",\"MM/dd/yyyy hh:mm:ss a\")\n",
        "\n",
        "\n",
        "# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\n",
        "df.createOrReplaceTempView(\"311_service_report\")\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "uG-j02YzvSAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Run to remove cache\n",
        "df.unpersist()"
      ],
      "metadata": {
        "id": "N22oTZo3vWF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that pyspark is set up and the columns of the dataset are updated to types that we expect, we can start using pyspark to explore and clean the dataset!"
      ],
      "metadata": {
        "id": "HfztP4LGvY0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Define path for US zip dataset\n",
        "# (Note: make sure to update to your netid and dataset name)\n",
        "uszip_path = \"/user/qyc206/uszips.csv\""
      ],
      "metadata": {
        "id": "D_H0IAiRvaPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "\n",
        "# Read the US zip dataset\n",
        "us = spark.read.csv(uszip_path, header=True)\n",
        "us = us.withColumn(\"zip\",us[\"zip\"].cast(IntegerType()))\n",
        "us = us.withColumn(\"lat\",us[\"lat\"].cast(DoubleType()))\n",
        "us = us.withColumn(\"lng\",us[\"lng\"].cast(DoubleType()))\n",
        "us.show()"
      ],
      "metadata": {
        "id": "MJxNPUaJvc-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Run to remove cache\n",
        "us.unpersist()"
      ],
      "metadata": {
        "id": "jPUpvzrXvhIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# All the cities in New york with it's zip code\n",
        "ny_data = (us.filter(us[\"state_id\"].like('NY'))).select(us[\"zip\"],us[\"city\"]) "
      ],
      "metadata": {
        "id": "GNsHJ7NcviVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Run to remove cache\n",
        "ny_data.unpersist()"
      ],
      "metadata": {
        "id": "vnm-gYtRvo4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cells to solve the inaccuracy problem."
      ],
      "metadata": {
        "id": "5brHhrSWvs3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "def calculate_distinct(col,dataoriginal,get_option=\"count\"):\n",
        "    distinct_vals = dataoriginal.select(col).distinct()\n",
        "    if get_option==\"count\":\n",
        "        return distinct_vals.count()\n",
        "    elif get_option == \"distinct\":\n",
        "        return distinct_vals"
      ],
      "metadata": {
        "id": "Uit9uwuyvq-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "def find_wrong_vals(correct_dataset,dataset_to_clean,col_in_correct,col_to_be_cleaned):\n",
        "    \"\"\"\n",
        "    Returns a list of wrong values in the dataset given that there is a dataset with all correct values\n",
        "    \"\"\"\n",
        "    col_vals = calculate_distinct(col_in_correct,correct_dataset,\"distinct\")\n",
        "    list_of_correct_vals = [row for row in col_vals.collect()]\n",
        "    vals_in_dataset = calculate_distinct(col_to_be_cleaned,dataset_to_clean,\"distinct\")\n",
        "    list_of_vals_in_dataset = [row for row in vals_in_dataset.collect()]\n",
        "    #Finding the incorrect names\n",
        "    list_of_wrong_vals = []\n",
        "    for i in list_of_vals_in_dataset:\n",
        "        if i not in list_of_correct_vals:\n",
        "            list_of_wrong_vals.append(i)\n",
        "    return list_of_correct_vals,list_of_wrong_vals"
      ],
      "metadata": {
        "id": "5ME-TBA7vvQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "def levenshteinDistance(s1, s2):\n",
        "    if len(s1) > len(s2):\n",
        "        s1, s2 = s2, s1\n",
        "\n",
        "    distances = range(len(s1) + 1)\n",
        "    for i2, c2 in enumerate(s2):\n",
        "        distances_ = [i2+1]\n",
        "        for i1, c1 in enumerate(s1):\n",
        "            if c1 == c2:\n",
        "                distances_.append(distances[i1])\n",
        "            else:\n",
        "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
        "        distances = distances_\n",
        "    return distances[-1]\n",
        "\n",
        "def clean_col(list_of_correct_vals,list_of_mispellings):\n",
        "    \"\"\"\n",
        "    calculates the levenshtein distance between each of the values of correct_vals and mispellings and finds the one that has minimum distance to be used as the new value\n",
        "    \"\"\"\n",
        "    list_new_values = {}\n",
        "    for i in list_of_mispellings:\n",
        "        matched = \"\"\n",
        "        min_val = 100000000\n",
        "        for j in list_of_correct_vals:\n",
        "            try:\n",
        "                curr_dist = levenshteinDistance(i[0], j[0])\n",
        "                if curr_dist<min_val:\n",
        "                    min_val =curr_dist\n",
        "                    matched = j[0]\n",
        "            except:\n",
        "                continue\n",
        "        list_new_values[str(i[0])] = str(matched)\n",
        "    return list_new_values\n",
        "            "
      ],
      "metadata": {
        "id": "0YR1uxk6vxL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "list_of_correct_cities,list_of_wrong_cities = find_wrong_vals(ny_data,df,\"city\",\"City\")\n",
        "print(\"Number of wrong cities before cleaning:\"+str(len(list_of_wrong_cities)))"
      ],
      "metadata": {
        "id": "sbey1ZYUv1Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "list_of_new_values = clean_col(list_of_correct_cities,list_of_wrong_cities)"
      ],
      "metadata": {
        "id": "sEJZYoXJv3Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code for profiling here so that we can use it to compare to cleaned results to see the improvement."
      ],
      "metadata": {
        "id": "kUxpE4Amv6WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "#This line of code shows you a dictionary of (valuesToBeCorrected:valueToCorrectTo), however, Running this code could cause heap issues when running future lines\n",
        "list_of_new_values"
      ],
      "metadata": {
        "id": "kWvMVCNsv8Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "keys = list(list_of_new_values.keys())\n",
        "values = list(list_of_new_values.values())\n",
        "df = df.replace(keys,values,\"City\")"
      ],
      "metadata": {
        "id": "KLQR8Fzsv-sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Run to remove cache\n",
        "df.unpersist()"
      ],
      "metadata": {
        "id": "RI5gnKLnwAIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# So that memory does not get overloaded\n",
        "list_of_new_values.clear()\n",
        "list_of_correct_cities.clear()\n",
        "keys.clear()\n",
        "values.clear()\n",
        "\n",
        "del list_of_new_values\n",
        "del list_of_correct_cities\n",
        "del keys\n",
        "del values"
      ],
      "metadata": {
        "id": "WXIN45sVwB-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}