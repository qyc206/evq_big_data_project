{
  "paragraphs": [
    {
      "user": "nbuser",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354797546_2014674091",
      "id": "paragraph_1639354797546_2014674091",
      "dateCreated": "2021-12-12T19:19:57-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:595",
      "text": "%md \n## V. Outlier\n\nAs shown in our profiling, there are several outliers in three columns of timestamp type: \"Closed Date\", \"Due Date\", and \"Resolution Action Updated Date\". We want to filter out rows with dates that should not belong in the dataset (aka any date from the year before 2010 and any date after 2021). We want to filter the data out rather than try to fix it because there would be no way to find out the correct dates for these outliers. \n\n**NOTE:** There is a count between each filtering to show the amount the filter affects the dataset.\n\nThe results are also shown after each filtering. It can be observed that the date ranges are now reasonable (i.e. between 2010 and present/2021). \n",
      "dateUpdated": "2021-12-12T20:00:02-0500",
      "dateFinished": "2021-12-12T20:00:02-0500",
      "dateStarted": "2021-12-12T20:00:02-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>V. Outlier</h2>\n<p>As shown in our profiling, there are several outliers in three columns of timestamp type: &ldquo;Closed Date&rdquo;, &ldquo;Due Date&rdquo;, and &ldquo;Resolution Action Updated Date&rdquo;. We want to filter out rows with dates that should not belong in the dataset (aka any date from the year before 2010 and any date after 2021). We want to filter the data out rather than try to fix it because there would be no way to find out the correct dates for these outliers.</p>\n<p><strong>NOTE:</strong> There is a count between each filtering to show the amount the filter affects the dataset.</p>\n<p>The results are also shown after each filtering. It can be observed that the date ranges are now reasonable (i.e. between 2010 and present/2021).</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n## Upload the dataset to Peel cluster & Define dataset path\n\nBefore continuing, make sure your dataset is available on Peel HDFS. If your dataset is on your local machine, you can copy them to the login node of the cluster and move them to your user directory in the HDFS using the following commands:\n\n```\n# Copy file from local machine to login node of the cluster\nmylaptop$ scp -r [FILENAME] <net_id>@peel.hpc.nyu.edu:~/\n\n# Move file from cluster login node to your user directory in HDFS \n# (your file will be in the path \"/user/[netid]/[FILENAME]\")\nhfs -put [FILENAME]\n```\n\nMake sure you can locate your dataset before continuing onwards.",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T20:00:04-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354809662_345660986",
      "id": "paragraph_1639354809662_345660986",
      "dateCreated": "2021-12-12T19:20:09-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:669",
      "dateFinished": "2021-12-12T20:00:04-0500",
      "dateStarted": "2021-12-12T20:00:04-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Upload the dataset to Peel cluster &amp; Define dataset path</h2>\n<p>Before continuing, make sure your dataset is available on Peel HDFS. If your dataset is on your local machine, you can copy them to the login node of the cluster and move them to your user directory in the HDFS using the following commands:</p>\n<pre><code># Copy file from local machine to login node of the cluster\nmylaptop$ scp -r [FILENAME] &lt;net_id&gt;@peel.hpc.nyu.edu:~/\n\n# Move file from cluster login node to your user directory in HDFS \n# (your file will be in the path &quot;/user/[netid]/[FILENAME]&quot;)\nhfs -put [FILENAME]\n</code></pre>\n<p>Make sure you can locate your dataset before continuing onwards.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n# Define path to dataset on Peel HDFS (NOTE: replace file name with your own if different)\ndataset_path = \"/user/CS-GY-6513/project_data/data-cityofnewyork-us.erm2-nwe9.csv\"",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:21:17-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354872805_712052575",
      "id": "paragraph_1639354872805_712052575",
      "dateCreated": "2021-12-12T19:21:12-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:849",
      "dateFinished": "2021-12-12T19:21:35-0500",
      "dateStarted": "2021-12-12T19:21:17-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%md\n\n## Set up Spark Session\n\nNow that the dataset is uploaded and the path is defined, we need to set up pyspark to begin profiling and exploring our dataset. \n\nIf this notebook is run in an environment where pyspark is not yet installed, please add a new cell BEFORE the next cell and run the following command:\n\n```\n# Run this command if pyspark is not already installed\n%pip install pyspark\n```\n",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T20:00:00-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354864330_308793194",
      "id": "paragraph_1639354864330_308793194",
      "dateCreated": "2021-12-12T19:21:04-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:768",
      "dateFinished": "2021-12-12T20:00:00-0500",
      "dateStarted": "2021-12-12T20:00:00-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Set up Spark Session</h2>\n<p>Now that the dataset is uploaded and the path is defined, we need to set up pyspark to begin profiling and exploring our dataset.</p>\n<p>If this notebook is run in an environment where pyspark is not yet installed, please add a new cell BEFORE the next cell and run the following command:</p>\n<pre><code># Run this command if pyspark is not already installed\n%pip install pyspark\n</code></pre>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Set up pyspark session\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n            .builder \\\n            .appName(\"Python Spark SQL basic example\") \\\n            .config(\"spark.some.config.option\", \"some-value\") \\\n            .config(\"spark.executor.memory\", \"35g\") \\\n            .config(\"spark.driver.memory\", \"35g\") \\\n            .getOrCreate()",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:21:39-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354888024_1060833820",
      "id": "paragraph_1639354888024_1060833820",
      "dateCreated": "2021-12-12T19:21:28-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:924",
      "dateFinished": "2021-12-12T19:21:40-0500",
      "dateStarted": "2021-12-12T19:21:39-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%md\n\n## Load dataset using spark\n\nRun the following lines to load the dataset using spark and test to make sure that dataset is properly loaded.\n",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:58:25-0500",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354899348_1543492961",
      "id": "paragraph_1639354899348_1543492961",
      "dateCreated": "2021-12-12T19:21:39-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1014",
      "dateFinished": "2021-12-12T19:21:47-0500",
      "dateStarted": "2021-12-12T19:21:47-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Load dataset using spark</h2>\n<p>Run the following lines to load the dataset using spark and test to make sure that dataset is properly loaded.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Load dataset\ndf = spark.read.format('csv').options(header='true',inferschema='true').load(dataset_path)\n# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\ndf.createOrReplaceTempView(\"311_service_report\") ",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:21:55-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=0",
              "$$hashKey": "object:2020"
            },
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=1",
              "$$hashKey": "object:2021"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354907412_2102715729",
      "id": "paragraph_1639354907412_2102715729",
      "dateCreated": "2021-12-12T19:21:47-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1098",
      "dateFinished": "2021-12-12T19:24:42-0500",
      "dateStarted": "2021-12-12T19:21:55-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%md\n### Generalizing Formatting\n\nFor many datasets, to optimally find information about any column that involves time, the column type must be turned into a timestamp type. However, to turn a column type into a timestamp, the data within the column must match the format that is specified when calling the to_timestamp() function ( to_timestamp(dataset[column], format) ). Therefore, it is best to be able to generalize this part of formating to make sure all our date columns are uniforom. This is even more essential since some of our solutions involve dates.",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:59:57-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354915495_363503468",
      "id": "paragraph_1639354915495_363503468",
      "dateCreated": "2021-12-12T19:21:55-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1179",
      "dateFinished": "2021-12-12T19:59:57-0500",
      "dateStarted": "2021-12-12T19:59:57-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Generalizing Formatting</h3>\n<p>For many datasets, to optimally find information about any column that involves time, the column type must be turned into a timestamp type. However, to turn a column type into a timestamp, the data within the column must match the format that is specified when calling the to_timestamp() function ( to_timestamp(dataset[column], format) ). Therefore, it is best to be able to generalize this part of formating to make sure all our date columns are uniforom. This is even more essential since some of our solutions involve dates.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\ndef formatDate(dataset, col, DateForm):\n    formatedData = dataset.withColumn(col,to_timestamp(dataset[col],DateForm))\n    return formatedData",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:24:49-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354922506_2010452890",
      "id": "paragraph_1639354922506_2010452890",
      "dateCreated": "2021-12-12T19:22:02-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1269",
      "dateFinished": "2021-12-12T19:24:49-0500",
      "dateStarted": "2021-12-12T19:24:49-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.types import IntegerType, DoubleType\nfrom pyspark.sql.functions import to_timestamp\n\n# Type casting to expected types\ndf = df.withColumn(\"Unique Key\",df[\"Unique Key\"].cast(IntegerType()))\ndf = formatDate(df,\"Due Date\",\"MM/dd/yyyy hh:mm:ss a\")\ndf = formatDate(df,\"Created Date\",\"MM/dd/yyyy hh:mm:ss a\")\ndf = formatDate(df,\"Closed Date\",\"MM/dd/yyyy hh:mm:ss a\")\ndf = df.withColumn(\"Incident Zip\",df[\"Incident Zip\"].cast(IntegerType()))\ndf = df.withColumn(\"BBL\",df[\"BBL\"].cast(IntegerType()))\ndf = df.withColumn(\"X Coordinate (State Plane)\",df[\"X Coordinate (State Plane)\"].cast(IntegerType()))\ndf = df.withColumn(\"Y Coordinate (State Plane)\",df[\"Y Coordinate (State Plane)\"].cast(IntegerType()))\ndf = formatDate(df,\"Resolution Action Updated Date\",\"MM/dd/yyyy hh:mm:ss a\")\n\n\n# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\ndf.createOrReplaceTempView(\"311_service_report\")\n\ndf.printSchema()",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:24:54-0500",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354931991_913351868",
      "id": "paragraph_1639354931991_913351868",
      "dateCreated": "2021-12-12T19:22:11-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1360",
      "dateFinished": "2021-12-12T19:24:51-0500",
      "dateStarted": "2021-12-12T19:24:51-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- Unique Key: integer (nullable = true)\n |-- Created Date: timestamp (nullable = true)\n |-- Closed Date: timestamp (nullable = true)\n |-- Agency: string (nullable = true)\n |-- Agency Name: string (nullable = true)\n |-- Complaint Type: string (nullable = true)\n |-- Descriptor: string (nullable = true)\n |-- Location Type: string (nullable = true)\n |-- Incident Zip: integer (nullable = true)\n |-- Incident Address: string (nullable = true)\n |-- Street Name: string (nullable = true)\n |-- Cross Street 1: string (nullable = true)\n |-- Cross Street 2: string (nullable = true)\n |-- Intersection Street 1: string (nullable = true)\n |-- Intersection Street 2: string (nullable = true)\n |-- Address Type: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Landmark: string (nullable = true)\n |-- Facility Type: string (nullable = true)\n |-- Status: string (nullable = true)\n |-- Due Date: timestamp (nullable = true)\n |-- Resolution Description: string (nullable = true)\n |-- Resolution Action Updated Date: timestamp (nullable = true)\n |-- Community Board: string (nullable = true)\n |-- BBL: integer (nullable = true)\n |-- Borough: string (nullable = true)\n |-- X Coordinate (State Plane): integer (nullable = true)\n |-- Y Coordinate (State Plane): integer (nullable = true)\n |-- Open Data Channel Type: string (nullable = true)\n |-- Park Facility Name: string (nullable = true)\n |-- Park Borough: string (nullable = true)\n |-- Vehicle Type: string (nullable = true)\n |-- Taxi Company Borough: string (nullable = true)\n |-- Taxi Pick Up Location: string (nullable = true)\n |-- Bridge Highway Name: string (nullable = true)\n |-- Bridge Highway Direction: string (nullable = true)\n |-- Road Ramp: string (nullable = true)\n |-- Bridge Highway Segment: string (nullable = true)\n |-- Latitude: double (nullable = true)\n |-- Longitude: double (nullable = true)\n |-- Location: string (nullable = true)\n\n"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Run to remove cache\ndf.unpersist()",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:24:58-0500",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354944454_1136144271",
      "id": "paragraph_1639354944454_1136144271",
      "dateCreated": "2021-12-12T19:22:24-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1429",
      "dateFinished": "2021-12-12T19:24:56-0500",
      "dateStarted": "2021-12-12T19:24:56-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[Unique Key: int, Created Date: timestamp, Closed Date: timestamp, Agency: string, Agency Name: string, Complaint Type: string, Descriptor: string, Location Type: string, Incident Zip: int, Incident Address: string, Street Name: string, Cross Street 1: string, Cross Street 2: string, Intersection Street 1: string, Intersection Street 2: string, Address Type: string, City: string, Landmark: string, Facility Type: string, Status: string, Due Date: timestamp, Resolution Description: string, Resolution Action Updated Date: timestamp, Community Board: string, BBL: int, Borough: string, X Coordinate (State Plane): int, Y Coordinate (State Plane): int, Open Data Channel Type: string, Park Facility Name: string, Park Borough: string, Vehicle Type: string, Taxi Company Borough: string, Taxi Pick Up Location: string, Bridge Highway Name: string, Bridge Highway Direction: string, Road Ramp: string, Bridge Highway Segment: string, Latitude: double, Longitude: double, Location: string]\n"
          }
        ]
      }
    },
    {
      "text": "%md\n\n## Cleaning\n\nNow that pyspark is set up and the columns of the dataset are updated to types that we expect, we can start using pyspark to explore and clean the dataset!",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:59:53-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354951319_782197525",
      "id": "paragraph_1639354951319_782197525",
      "dateCreated": "2021-12-12T19:22:31-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1498",
      "dateFinished": "2021-12-12T19:59:53-0500",
      "dateStarted": "2021-12-12T19:59:53-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Cleaning</h2>\n<p>Now that pyspark is set up and the columns of the dataset are updated to types that we expect, we can start using pyspark to explore and clean the dataset!</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import min, max",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:28:23-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354952922_1769270141",
      "id": "paragraph_1639354952922_1769270141",
      "dateCreated": "2021-12-12T19:22:32-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1573",
      "dateFinished": "2021-12-12T19:28:23-0500",
      "dateStarted": "2021-12-12T19:28:23-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%md \n\nThis specific error is one of the more prevalent errors found amongst the datasets. Therefore, we provide here a generalized version of the code that allows this simple error fix to be run on other datasets given the dataframe, min and max dates, and column name.\n\nTo also work for specific cases where we only need to filter min OR max dates and not both, specific functions were made to deal with this",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T20:00:18-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354971922_726799530",
      "id": "paragraph_1639354971922_726799530",
      "dateCreated": "2021-12-12T19:22:51-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1654",
      "dateFinished": "2021-12-12T20:00:18-0500",
      "dateStarted": "2021-12-12T20:00:18-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>This specific error is one of the more prevalent errors found amongst the datasets. Therefore, we provide here a generalized version of the code that allows this simple error fix to be run on other datasets given the dataframe, min and max dates, and column name.</p>\n<p>To also work for specific cases where we only need to filter min OR max dates and not both, specific functions were made to deal with this</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n\n### Generalized functions",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T20:00:40-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355841319_1482786858",
      "id": "paragraph_1639355841319_1482786858",
      "dateCreated": "2021-12-12T19:37:21-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2975",
      "dateFinished": "2021-12-12T20:00:40-0500",
      "dateStarted": "2021-12-12T20:00:40-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Generalized functions</h3>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\ndef removeOutlierDates(df,minDate,maxDate, col):\n    df = df.filter(df[col].isNull() | (year(col) >= minDate) & (year(col) <= maxDate))\n    return df\n\ndef filterMinOnlyDates(df,minDate,maxDate, col):\n    df = df.filter(df[col].isNull() | (year(col) >= minDate))\n    return df\n\ndef filterMaxOnlyDates(df, maxDate, col):\n    df = df.filter(df[col].isNull() |  (year(col) <= maxDate))\n    return df",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:28:27-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354975466_2087480332",
      "id": "paragraph_1639354975466_2087480332",
      "dateCreated": "2021-12-12T19:22:55-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1729",
      "dateFinished": "2021-12-12T19:28:27-0500",
      "dateStarted": "2021-12-12T19:28:27-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%md \n\n### Closed Date column",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T20:01:13-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639357271748_549473833",
      "id": "paragraph_1639357271748_549473833",
      "dateCreated": "2021-12-12T20:01:11-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:3548",
      "dateFinished": "2021-12-12T20:01:13-0500",
      "dateStarted": "2021-12-12T20:01:13-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Closed Date column</h3>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Fixing dates from Closed Date\nfrom pyspark.sql.functions import year, desc\n\ndf = removeOutlierDates(df, 2018, 2021, \"Closed Date\")",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:29:39-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639354996024_1310997875",
      "id": "paragraph_1639354996024_1310997875",
      "dateCreated": "2021-12-12T19:23:16-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1810",
      "dateFinished": "2021-12-12T19:29:39-0500",
      "dateStarted": "2021-12-12T19:29:39-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%pyspark\n\n# Display results\ndf.select(min(\"Closed Date\"),max(\"Closed Date\")).show(df.count(), False)",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:29:52-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=2",
              "$$hashKey": "object:2234"
            },
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=3",
              "$$hashKey": "object:2235"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355005257_1486104306",
      "id": "paragraph_1639355005257_1486104306",
      "dateCreated": "2021-12-12T19:23:25-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1879",
      "dateFinished": "2021-12-12T19:33:57-0500",
      "dateStarted": "2021-12-12T19:29:52-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------------------+-------------------+\n|min(Closed Date)   |max(Closed Date)   |\n+-------------------+-------------------+\n|2018-01-01 00:00:00|2021-11-18 12:00:00|\n+-------------------+-------------------+\n\n"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Count of the number of overall rows currently in the data\ndf.count()",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:34:08-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=4",
              "$$hashKey": "object:2817"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355490012_997696307",
      "id": "paragraph_1639355490012_997696307",
      "dateCreated": "2021-12-12T19:31:30-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2139",
      "dateFinished": "2021-12-12T19:36:05-0500",
      "dateStarted": "2021-12-12T19:34:08-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "10732896\n"
          }
        ]
      }
    },
    {
      "text": "%md\n\n### Due Date column",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:34:55-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355683379_1539288143",
      "id": "paragraph_1639355683379_1539288143",
      "dateCreated": "2021-12-12T19:34:43-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2420",
      "dateFinished": "2021-12-12T19:34:55-0500",
      "dateStarted": "2021-12-12T19:34:55-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Due Date column</h3>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Fixing dates from Due Dates\nfrom pyspark.sql.functions import year, desc\n\ndf = removeOutlierDates(df, 2018, 2021, \"Due Date\")",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:36:37-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355648223_1425892963",
      "id": "paragraph_1639355648223_1425892963",
      "dateCreated": "2021-12-12T19:34:08-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2244",
      "dateFinished": "2021-12-12T19:36:37-0500",
      "dateStarted": "2021-12-12T19:36:37-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%pyspark\n\n# Display results\ndf.select(min(\"Due Date\"),max(\"Due Date\")).show(df.count(), False)",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:36:41-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=5",
              "$$hashKey": "object:3088"
            },
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=6",
              "$$hashKey": "object:3089"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355722096_1737468881",
      "id": "paragraph_1639355722096_1737468881",
      "dateCreated": "2021-12-12T19:35:22-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2507",
      "dateFinished": "2021-12-12T19:40:56-0500",
      "dateStarted": "2021-12-12T19:36:41-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------------------+-------------------+\n|min(Due Date)      |max(Due Date)      |\n+-------------------+-------------------+\n|2018-01-01 00:01:58|2021-12-18 01:25:39|\n+-------------------+-------------------+\n\n"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Count of the number of overall rows currently in the data to check we didn't get rid of too many rows\ndf.count()",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:41:00-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=7",
              "$$hashKey": "object:3134"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355733765_2075526199",
      "id": "paragraph_1639355733765_2075526199",
      "dateCreated": "2021-12-12T19:35:33-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2576",
      "dateFinished": "2021-12-12T19:43:07-0500",
      "dateStarted": "2021-12-12T19:41:00-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "10445112\n"
          }
        ]
      }
    },
    {
      "text": "%md\n\n### Resolution Action Updated Date column",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:36:00-0500",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355744424_2075950580",
      "id": "paragraph_1639355744424_2075950580",
      "dateCreated": "2021-12-12T19:35:44-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2645",
      "dateFinished": "2021-12-12T19:36:00-0500",
      "dateStarted": "2021-12-12T19:36:00-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Resolution Action Updated Date column</h3>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Fixing dates from Resolution Action Updated Date\nfrom pyspark.sql.functions import year, desc\n\ndf = removeOutlierDates(df, 2018, 2021, \"Resolution Action Updated Date\")",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:43:39-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355760815_36133671",
      "id": "paragraph_1639355760815_36133671",
      "dateCreated": "2021-12-12T19:36:00-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2720",
      "dateFinished": "2021-12-12T19:43:39-0500",
      "dateStarted": "2021-12-12T19:43:39-0500",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%pyspark\n\n# Display results\ndf.select(min(\"Resolution Action Updated Date\"),max(\"Resolution Action Updated Date\")).show(df.count(), False)",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:43:43-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=8",
              "$$hashKey": "object:3196"
            },
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=9",
              "$$hashKey": "object:3197"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355790554_1479268327",
      "id": "paragraph_1639355790554_1479268327",
      "dateCreated": "2021-12-12T19:36:30-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2822",
      "dateFinished": "2021-12-12T19:49:03-0500",
      "dateStarted": "2021-12-12T19:43:43-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------------------------+-----------------------------------+\n|min(Resolution Action Updated Date)|max(Resolution Action Updated Date)|\n+-----------------------------------+-----------------------------------+\n|2018-01-01 00:00:00                |2021-11-18 12:00:00                |\n+-----------------------------------+-----------------------------------+\n\n"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n\n# Count of the number of overall rows currently in the data\ndf.count()",
      "user": "nbuser",
      "dateUpdated": "2021-12-12T19:49:13-0500",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://hc15.nyu.cluster:41567/jobs/job?id=10",
              "$$hashKey": "object:3320"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639355816138_1434464996",
      "id": "paragraph_1639355816138_1434464996",
      "dateCreated": "2021-12-12T19:36:56-0500",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2906",
      "dateFinished": "2021-12-12T19:51:48-0500",
      "dateStarted": "2021-12-12T19:49:13-0500",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "10299590\n"
          }
        ]
      }
    }
  ],
  "name": "Cleaning_Dataset_peel_Outlier",
  "id": "2GRHCGHHA",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Cleaning_Dataset_peel_Outlier"
}