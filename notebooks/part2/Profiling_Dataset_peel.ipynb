{
  "metadata": {
    "name": "Profiling_Dataset",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Profiling \n\nThis notebook goes over the problems that we found while exploring the sample dataset (i.e. the sample dataset with ~5 million rows). We broke down the problems that we found into the following sections: Uniformity, Accuracy, Inconsistency, Completeness, and Outlier. Each section will contain cells that can be run to explore the dataset and observe the problems that we found.\n\nThis notebook is also used to profile the datasets that are found to be similar to the initial 311 service report dataset profiled and cleaned in part one of the project. "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Upload the dataset to Peel cluster \u0026 Define dataset path\n\nBefore continuing, make sure your dataset is available on Peel HDFS. If your dataset is on your local machine, you can copy them to the login node of the cluster and move them to your user directory in the HDFS using the following commands:\n\n```\n# Copy file from local machine to login node of the cluster\nmylaptop$ scp -r [FILENAME] \u003cnet_id\u003e@peel.hpc.nyu.edu:~/\n\n# Move file from cluster login node to your user directory in HDFS \n# (your file will be in the path \"/user/[netid]/[FILENAME]\")\nhfs -put [FILENAME]\n```\n\nMake sure you can locate your dataset before continuing onwards."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Define path to dataset on Peel HDFS (NOTE: replace file name with your own if different)\ndataset_path \u003d \"/user/CS-GY-6513/project_data/data-cityofnewyork-us.erm2-nwe9.csv\""
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Set up Spark Session\n\nNow that the dataset is uploaded and the path is defined, we need to set up pyspark to begin profiling and exploring our dataset. \n\nIf this notebook is run in an environment where pyspark is not yet installed, please add a new cell BEFORE the next cell and run the following command:\n\n```\n# Run this command if pyspark is not already installed\n%pip install pyspark\n```\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark \n\n# Set up pyspark session\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession \\\n            .builder \\\n            .appName(\"Python Spark SQL basic example\") \\\n            .config(\"spark.some.config.option\", \"some-value\") \\\n            .config(\"spark.executor.memory\", \"20g\") \\\n            .config(\"spark.driver.memory\", \"20g\") \\\n            .getOrCreate()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**WARNING: If you run into a java heap memory, configure the following lines in the cell above:\n\n.config(\"spark.executor.memory\", \"30g\")\n.config(\"spark.driver.memory\", \"30g\")\nChange the number infront of the g (ex:20g)\n\nChanging this number could also change the amount of RAM needed to download the final file\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Load dataset using spark\n\nRun the following lines to load the dataset using spark and test to make sure that dataset is properly loaded."
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Load dataset\ndf \u003d spark.read.format(\u0027csv\u0027).options(header\u003d\u0027true\u0027,inferschema\u003d\u0027true\u0027).load(dataset_path)\n# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\ndf.createOrReplaceTempView(\"311_service_report\") "
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Check if the dataset is properly loaded by printing its schema\ndf.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Notice in the result of running the above cell that most items in the schema is of type string, even if it is not the expected type. To modify the dataset such that the types of each column is what we would expect, we perform type casting for each column that should not be type string.\n\n**NOTE: the following cell is specific for the 311 service report dataset; make sure to modify the following cell to include type casting that is necessary to your dataset, if different"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.types import IntegerType, DoubleType\nfrom pyspark.sql.functions import to_timestamp\n\n# Type casting to expected types\ndf \u003d df.withColumn(\"Unique Key\",df[\"Unique Key\"].cast(IntegerType()))\ndf \u003d df.withColumn(\"Due Date\",to_timestamp(df[\"Due Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\ndf \u003d df.withColumn(\"Created Date\", to_timestamp(df[\"Created Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\ndf \u003d df.withColumn(\"Closed Date\",to_timestamp(df[\"Closed Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\ndf \u003d df.withColumn(\"Incident Zip\",df[\"Incident Zip\"].cast(IntegerType()))\ndf \u003d df.withColumn(\"BBL\",df[\"BBL\"].cast(IntegerType()))\ndf \u003d df.withColumn(\"X Coordinate (State Plane)\",df[\"X Coordinate (State Plane)\"].cast(IntegerType()))\ndf \u003d df.withColumn(\"Y Coordinate (State Plane)\",df[\"Y Coordinate (State Plane)\"].cast(IntegerType()))\ndf \u003d df.withColumn(\"Resolution Action Updated Date\",to_timestamp(df[\"Resolution Action Updated Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n\n# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\ndf.createOrReplaceTempView(\"311_service_report\")\n\ndf.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Now that pyspark is set up and the columns of the dataset are updated to types that we expect, we can start using pyspark to explore the dataset!"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## I. Uniformity\n\nWe describe uniform data as data that use the same format and unit of measure. As we explored the 311 service report dataset, we found five columns with values that have non-uniform casing (i.e. some values are fully uppercased, some are fully lowercased, and some only have the first letter of each word uppercased).\n\nRun through the following cells to observe the problem in the five columns.\n\n**NOTE: to profile for uniformity in columns specific for your dataset, run the following command: \n```\n# Replace the [COLUMN_NAME] with the column that you want to profile\ndf.select([COLUMN_NAME]).distinct().collect()\n\n# Use .head([NUMBER_OF_ROWS_TO_DISPLAY]) instead if the result is too large for display\ndf.select([COLUMN_NAME]).distinct().head(20)\n```"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Run the following cell and notice that \"Complaint Type\" column has values that have non-uniform casing (ex: one row is \"Traffic Signal Condition\" while another row is \"SAFETY\"). Additionally, several unexpected, incorrect values can be observed, but this will be further investigated in the Accuracy section. "
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View column \"Complaint Type\" \ndf.select(\"Complaint Type\").distinct().collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Run the following cell and notice that the \"Descriptor\" column has the same problem of non-uniform casing, as previously observed in \"Complaint Type\" column. If observed carefully, a row with \"Sidewalk CafÃ©\" can be found. This problem will brought up again in the Accuracy section."
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View column \"Descriptor\" \ndf.select(\"Descriptor\").distinct().collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Run the following cell and notice that the \"Location Type\" column is generally uniform except for the rows with the value \"RESIDENTIAL BUILDING\", \"3+ Family ApT\", and \"3+ Family Apt.\"."
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View column \"Location Type\"\ndf.select(\"Location Type\").distinct().collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Run the following cell and notice that the \"Street Name\" column has non-uniform casing as well (ex: one row has \"Lenox Road\" while majority of the rows are uppercased like \"ALBERTA AVE\")."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View column \"Street Name\" \ndf.select(\"Street Name\").distinct().head(200)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Run the following cell and notice that the \"City\" column has nonuniform casing (ex: one row has \"New York\" while majority of the rows are all uppercased like \"ROCKVILLE CENTER\")."
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View column \"City\"\ndf.select(\"City\").distinct().collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## II. Accuracy\n\nWe describe accurate data as data that is close to the true, expected values. As we explored our dataset, we found that there are several inaccurate cities in the city column. To explore the accuracy in the city column, we found and downloadeded a dataset ([uszips.csv](https://drive.google.com/file/d/1qd2cXgTx-h-hRd0C7z2s_U4O8VYLAXA7/view?usp\u003dsharing)) that contains information such as US zipcode, state name, city, etc. We use this dataset as baseline to compare to our dataset to find any inaccuracies, such as misspellings. \n\nBefore we start, make sure to download and upload the [uszips.csv](https://drive.google.com/file/d/1qd2cXgTx-h-hRd0C7z2s_U4O8VYLAXA7/view?usp\u003dsharing) dataset into HDFS just as described in the previous \"Upload the dataset to Peel cluster \u0026 Define dataset path\" section. After the reference dataset is downloaded and uploaded into HDFS, run the cells to define the dataset path and make sure the dataset can be read."
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Define path for US zip dataset\n# (Note: make sure to update to your netid and dataset name)\nuszip_path \u003d \"/user/qyc206/uszips.csv\""
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Read the US zip dataset\nus \u003d spark.read.csv(uszip_path, header\u003dTrue)\nus \u003d us.withColumn(\"zip\",us[\"zip\"].cast(IntegerType()))\nus \u003d us.withColumn(\"lat\",us[\"lat\"].cast(DoubleType()))\nus \u003d us.withColumn(\"lng\",us[\"lng\"].cast(DoubleType()))\nus.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Once the dataset is uploaded, run the following cells to observe the problem.\n\n**NOTE: if your city column has a different column name, update \"City\" to your column name"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exploring City column"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# All distinct cities present in the dataset\ntemp_df_1 \u003d df.select(\"City\").distinct() \nlist_of_cities \u003d list(temp_df_1.toPandas()[\u0027City\u0027]) \n\n# All cities present in the US zip dataset\nlist_of_correct_cities \u003d  list(us.select(\"city\").distinct().toPandas()[\u0027city\u0027]) "
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nlist_of_wrong_cities \u003d []\n\nfor i in list_of_cities:\n  if i not in list_of_correct_cities:\n    list_of_wrong_cities.append(i)\n\n# First 20 mispelled words\nlist_of_wrong_cities[:20] "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "As we can observe from the results of running the cells, there are inaccuracies in the cities that are listed in the city column of our 311 service dataset. For instance, \"NEW JERSEY\" is a state, not a city and \"HUSTON\" is a misspelling for Houston, a city in Texas."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exploring Complaint Type column"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Another problem with accuracy that we find is in the \"Complaint Type\" column, as observed also during the profiling for uniformity problems. Run the following cells to further explore this problem.\n\n**NOTE: you can replace \"Complaint Type\" in the following cell with your specific column name to search for accuracy problems in your dataset, if different \u0026 if you are using your a different column for your own dataset, make sure to modify the pattern (of valid symbols) based on your expected contents."
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Define regular expression pattern\npattern \u003d \"^[-a-zA-Z0-9\\s\\(\\)\\.\\/]*$\""
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Get distinct values of column \"Complaint Type\" \ndf_distinct_complaints \u003d df.select(\"Complaint Type\").distinct()"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View rows with special characters\ndf_distinct_complaints.filter(~col(\"Complaint Type\").rlike(pattern)).collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exploring Descriptor column\n\nRun the following cell to observe similar problems as shown for the Complaint Type column. \n\nNOTE: make sure to run the first cell under the previous \"Exploring Complaint Type column\" section BEFORE running the following cells \u0026 if you are using your a different column for your own dataset, make sure to modify the pattern (of valid symbols) based on your expected contents."
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Define regular expression pattern\npattern2 \u003d \"^[-a-zA-Z0-9\\s\\(\\)\\.\\/\\,\\:\\*\\\u0027\\\u0026\\\"]*$\""
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Get distinct values of column \"Descriptor\" \ndf_distinct_descriptors \u003d df.select(\"Descriptor\").distinct()"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View rows with special characters\ndf_distinct_descriptors.filter(~col(\"Descriptor\").rlike(pattern2)).collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## III. Inconsistency\n\nWe describe inconsistent data as data that contains values that contradict each other or contains value that is not what we would expect based on the column that it is in. As we explored our dataset, we observed inconsistency problems in both the Agency and Agency Name columns in the 311 service dataset. We attempted to find a dataset that might contain all agency names and information, but we could not find a complete dataset. Instead, we decided to create our own custom dataset using the data from https://www1.nyc.gov/nyc-resources/agencies.page along with the addition of other agencies that showed up on the NYC open data website. Our dataset can be downloaded/accessed via this link: [NYC-Agency-Names.csv](https://drive.google.com/file/d/1EHpyXNOwCpv-NM0OTTqFBHeKikKpBtd1/view?usp\u003dsharing). We use this dataset as baseline to compare to the corresponding columns in our 311 service dataset. "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Just as before, make sure to download and upload the [NYC-Agency-Names.csv](https://drive.google.com/file/d/1EHpyXNOwCpv-NM0OTTqFBHeKikKpBtd1/view?usp\u003dsharing) dataset into HDFS just as described in the previous “Upload the dataset to Peel cluster \u0026 Define dataset path” section. After the reference dataset is downloaded and uploaded into HDFS, run the cells to define the dataset path and make sure the dataset can be read."
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Define path for NYC Agency Names dataset\n# (Note: make sure to update to your netid and dataset name)\nnycAgencyNames_path \u003d \"/user/qyc206/NYC-Agency-Names.csv\""
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Read NYC Agency Names dataset \nagency_df \u003d spark.read.csv(nycAgencyNames_path, header\u003dTrue)\n\n# Show Schema for NYC Agency Names dataset\nagency_df.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Once the dataset is uploaded, run the following cells to profile for problems.\n\n**NOTE: the following approaches can be used on corresponding columns from similar datasets; but make sure to change the column name accordingly in the cells"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exploring Agency column"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View data from Agency column\nfrom pyspark.sql.functions import asc\n\ndf.select(\"Agency\").distinct().orderBy(asc(\"Agency\")).show(df.count(), False)"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# All distinct Agency present in the dataset\ntemp_df2 \u003d df.select(\"Agency\").distinct() \nlist_of_agencies \u003d list(temp_df2.toPandas()[\u0027Agency\u0027]) \n\n# All Agency present in the NYC Agency Names dataset\nlist_of_correct_agencies \u003d list(agency_df.select(\"Agency Acronym\").distinct().toPandas()[\u0027Agency Acronym\u0027]) "
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Show list of Agency that were not in the dataset with uniform names\nlist_of_wrong_agencies \u003d []\n\nfor i in list_of_agencies:\n  if i not in list_of_correct_agencies:\n    list_of_wrong_agencies.append(i)\n\n# Show the first 20 in the list\nlist_of_wrong_agencies[:20] "
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Count how many rows have MAYORâ in the Agency column\ndf.filter(df[\"Agency\"][0:6] \u003d\u003d \"MAYORâ\").count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "As we can observe from the results of running the previous cells, there is inconsistent data where every other value in the column in an acronym, but we also have \"MAYORâ\\x80\\x99S OFFICE OF SPECIAL ENFORCEMENT\", which is not an acronym. By counting, we find that the number of rows in Agency column that has \"MAYORâ\" as part of the value is 70,685. Theoretically, we would solve the issue by finding out if the corresponding \"Agency Name\" is in our database, but for this smaller dataset and for simpliciy sake, we plan to just filter these values out.\n\nAdditionally, there is one minor problem where the apostrophe in \"Mayor\u0027s\" did not store correctly and turned into gibberish. Since this only happens in one specific case of \"MAYORâS OFFICE OF SPECIAL ENFORCEMENT\", the solution will be very specific to fix this problem.\n\nThere also might be some inaccuracies which resulted in the other values like \"TAX\" and \"CEO\". Note that even though \"3-1-1\" showed up in the list above, it is actually correct; this is because it is listed in the NYC Agency Names dataset as \"311\" without the dashes."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exploring Agency Name column"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View data from Agency Name column\ndf.select(\"Agency Name\").distinct().orderBy(asc(\"Agency Name\")).show(df.count(), False)"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# All distinct Agency Name present in the dataset\ntemp_df3 \u003d df.select(\"Agency Name\").distinct() \nlist_of_agency_names \u003d list(temp_df3.toPandas()[\u0027Agency Name\u0027]) \n\n# All Agency present in the NYC Agency Names dataset\nlist_of_correct_agency_names\u003d  list(agency_df.select(\"Agency\").distinct().toPandas()[\u0027Agency\u0027]) "
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Show list of non uniform names in the Agency Name Column\nlist_of_wrong_agency_names \u003d []\n\nfor i in list_of_agency_names:\n  if (i[0:6] !\u003d \"School\") and i not in list_of_correct_agency_names:\n    list_of_wrong_agency_names.append(i)\n\n# Show entire list\nfor j in list_of_wrong_agency_names:\n  print(j)"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Count how many rows have \"School\" as part of the value in the Agency Name column\ndf.filter((df[\"Agency Name\"][0:6] \u003d\u003d \"School\")).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Count how many rows have \"School\" as part of the value in Agency Name and \"DOE\" in Agency column\ndf.filter((df[\"Agency Name\"][0:6] \u003d\u003d \"School\") \u0026 (df[\"Agency\"] \u003d\u003d \"DOE\")).count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "As we can observe from the results above, there are several inconsistencies and other problems, including misspellings, uneven uppercase/lowercase usage, different titles (ex: \"The Department of...\" vs \"Department of...\"), and acronyms are being used despite there being an actual name to the department (ex: NYPD vs using New York Police Department). There are also many schools that are listed that are not technically agencies due to the DOE. However, for this specific case, if a school is listed with DOE for the Agency in the same row, we will leave this alone as we know this is most likely correct because we can observe that the number of agency names that are schools is equivalent to the number of values that have agency name that is a school and agency that is DOE. This school data is too detailed and can potentially be used for other purposes, so we decided that it should not be taken out of the dataset."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## IV. Completeness\n\nWe describe a complete dataset as a dataset that contains all required data. While exploring our dataset, we found that several values in the city column are null (i.e. missing).\n\nRun the following cells to view the observation.\n\n**NOTE: the following approaches can be used on corresponding columns from similar datasets; but make sure to change the column name accordingly in the cells"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exploring City column"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import desc\n\n# Display number of values in city column\ndf.groupBy(\u0027City\u0027).count().orderBy(desc(\"count\")).show(df.count(), False)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "From the results above, we can see that there are 1,690,360 rows in the City column that contain null values."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exploring Address Type column\n\nAnother factor of Completeness that we should keep a look out for involves the Address Type Column. This Column is integral for figuring out what columns holds the location information we are looking for. For example, the address type \"ADDRESS\" Most likely corresponds with the Incident Address field.\n\nTo figure out what Address Type corresponds to what location values, we would have to map out the amount of times non null values appeared in each column per different type. If there is a significant amount of data that appears when calling a certain address type, then that column most likely is associated with that type.\n\nIdeally, this Address Type Column should not be null, and if it is, then the columns that could hold information about the Address should also be null.\n\nIf the Address Type Column is not NULL, then we should also be checking if the correct Address Columns are filled while the rest of the address columns are null."
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Show the distinct address types\ndf.select(\"Address Type\").distinct().show(df.count(), False)"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import desc\n\n# Count how many rows of each address types there are\ndf.groupBy(\u0027Address Type\u0027).count().orderBy(desc(\"count\")).show(df.count(), False)"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Check to see if Incident Addressing is associated with the ADDRESS type\ndf.filter((df[\"Address Type\"] \u003d\u003d \"ADDRESS\") \u0026 df[\"Incident Address\"].isNotNull()).count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## V. Outlier\n\nWe describe an outlier as data that is significantly different from all other data in the dataset. To find potential outliers, we looked at the minimum and maximum dates in columns with timestamp types (i.e. \"Created Date\", \"Closed Date\", \"Due Date\", and \"Resolution Action Updated Date\"). We found outliers in three of the four columns with timestamp types.\n\nRun the following cells to explore the columns with timestamp types.\n\n**NOTE: the following approaches can be used on corresponding columns from similar datasets; but make sure to change the column name accordingly in the cells"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Import libraries\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import min, max"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\"Created Date\" column seems to have the expected minimum and maximum values for dates in our 311 service dataset. Run the following cell to view this observation."
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View min and max dates for Created Date\ndf.select(min(\"Created Date\"),max(\"Created Date\")).show(df.count(), False)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\"Closed Date\", \"Due Date\", and \"Resolution Action Updated Date\" columns can be observed to have outlier dates that clearly should not exist in our dataset. Run the following cells to view this observation."
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View min and max dates for Closed Date\ndf.select(min(\"Closed Date\"),max(\"Closed Date\")).show(df.count(), False)"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View min and max dates for Due Date\ndf.select(min(\"Due Date\"),max(\"Due Date\")).show(df.count(), False)"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# View min and max dates for Resolution Action Updated Date\ndf.select(min(\"Resolution Action Updated Date\"),max(\"Resolution Action Updated Date\")).show(df.count(), False)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conclusion\n\nIn this notebook, we explored different aspects of our dataset and uncovered problems with the dataset that needs to be cleaned and improved. The next step is actually trying to clean and create a new version of the dataset."
    }
  ]
}