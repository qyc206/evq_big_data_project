{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_Dataset_peel_Uniformity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ****IMPORTANT NOTE:**\n",
        "\n",
        "This notebook is for VIEW ONLY. To test and run the notebook, please download, upload and run the zeppelin notebook on Peel: [Cleaning_Dataset_peel_Uniformity.zpln](https://github.com/qyc206/evq_big_data_project/blob/main/notebooks/part2/Cleaning_Dataset_peel_Uniformity.zpln)."
      ],
      "metadata": {
        "id": "DK5hdF6_Uxtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the dataset to Peel cluster & Define dataset path\n",
        "\n",
        "Before continuing, make sure your dataset is available on Peel HDFS. If your dataset is on your local machine, you can copy them to the login node of the cluster and move them to your user directory in the HDFS using the following commands:\n",
        "\n",
        "```\n",
        "# Copy file from local machine to login node of the cluster\n",
        "mylaptop$ scp -r [FILENAME] <net_id>@peel.hpc.nyu.edu:~/\n",
        "\n",
        "# Move file from cluster login node to your user directory in HDFS \n",
        "# (your file will be in the path \"/user/[netid]/[FILENAME]\")\n",
        "hfs -put [FILENAME]\n",
        "```\n",
        "\n",
        "Make sure you can locate your dataset before continuing onwards."
      ],
      "metadata": {
        "id": "rlO3SOwyU1_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xar6Xw29UtHG"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Define path to dataset on Peel HDFS (NOTE: replace file name with your own if different)\n",
        "dataset_path = \"/user/CS-GY-6513/project_data/data-cityofnewyork-us.erm2-nwe9.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Set up pyspark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .appName(\"Python Spark SQL basic example\") \\\n",
        "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "            .config(\"spark.executor.memory\", \"50g\") \\\n",
        "            .config(\"spark.driver.memory\", \"50g\") \\\n",
        "            .getOrCreate()"
      ],
      "metadata": {
        "id": "Z0v3eMTHU9Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset using spark\n",
        "\n",
        "Run the following lines to load the dataset using spark and test to make sure that dataset is properly loaded."
      ],
      "metadata": {
        "id": "vL4m9YtRU-4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "# Load dataset\n",
        "df = spark.read.format('csv').options(header='true',inferschema='true').load(dataset_path)\n",
        "# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\n",
        "df.createOrReplaceTempView(\"311_service_report\") "
      ],
      "metadata": {
        "id": "jHmDM2qMVBSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the result of running the above cell that most items in the schema is of type string, even if it is not the expected type. To modify the dataset such that the types of each column is what we would expect, we perform type casting for each column that should not be type string.\n",
        "\n",
        "**NOTE: the following cell is specific for the 311 service report dataset; make sure to modify the following cell to include type casting that is necessary to your dataset, if different"
      ],
      "metadata": {
        "id": "lzGWEdsjVC43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Type casting to expected types\n",
        "df = df.withColumn(\"Unique Key\",df[\"Unique Key\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Due Date\",to_timestamp(df[\"Due Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Created Date\", to_timestamp(df[\"Created Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Closed Date\",to_timestamp(df[\"Closed Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "df = df.withColumn(\"Incident Zip\",df[\"Incident Zip\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"BBL\",df[\"BBL\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"X Coordinate (State Plane)\",df[\"X Coordinate (State Plane)\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Y Coordinate (State Plane)\",df[\"Y Coordinate (State Plane)\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"Resolution Action Updated Date\",to_timestamp(df[\"Resolution Action Updated Date\"],\"MM/dd/yyyy hh:mm:ss a\"))\n",
        "\n",
        "\n",
        "# (Note: change \"311_service_report\" to a name that better suits your dataset, if different)\n",
        "df.createOrReplaceTempView(\"311_service_report\")"
      ],
      "metadata": {
        "id": "pDbKLZWyVGLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Uniformity\n",
        "\n",
        "As observed during profiling, there are several non-uniform casing problems in the values of columns of type string. The problem is narrowed to the following five columns: \"Complaint Type\", \"Descriptor\", \"Location Type\", \"Street Name\", and \"City\". To solve this problem, we write a function called **oneColUniformCasing** that takes in a column name (type string) and updates the values of the items in the column to the format where the first character in every word is uppercased. \n",
        "\n",
        "Run the following cell with the function definition."
      ],
      "metadata": {
        "id": "COl-QOOuVJ9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "def calculate_distinct(col,dataoriginal,get_option=\"count\"):\n",
        "    distinct_vals = dataoriginal.select(col).distinct()\n",
        "    if get_option==\"count\":\n",
        "        return distinct_vals.count()\n",
        "    elif get_option == \"distinct\":\n",
        "        return distinct_vals"
      ],
      "metadata": {
        "id": "6QNjMbhbVNNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark \n",
        "from pyspark.sql.functions import initcap, col, trim\n",
        "def oneColUniformCasing(col_name,dataoriginal):\n",
        "    dataoriginal = dataoriginal.select(\"*\", trim(initcap(col(col_name))).alias('Temp name'))\n",
        "    dataoriginal  = dataoriginal.drop(col_name)\n",
        "    newdata  = dataoriginal.withColumnRenamed(\"Temp name\",col_name)\n",
        "    return newdata"
      ],
      "metadata": {
        "id": "fjt6SRshVRYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function is applied to the columns that are found to be non-uniform."
      ],
      "metadata": {
        "id": "4_cMhv0EVTYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# Apply oneColUniformCasing to \"Complaint Type\"\n",
        "df = oneColUniformCasing(\"Complaint Type\", df)\n",
        "\n",
        "# Apply oneColUniformCasing to \"Descriptor\"\n",
        "df = oneColUniformCasing(\"Descriptor\", df)\n",
        "\n",
        "# Apply oneColUniformCasing to \"Street Name\"\n",
        "df = oneColUniformCasing(\"Street Name\", df)\n",
        "\n",
        "# Apply oneColUniformCasing to \"City\"\n",
        "df = oneColUniformCasing(\"City\", df)"
      ],
      "metadata": {
        "id": "1rImiUS_VVKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since “Location Type” column only has one nonuniform value found, this value is directly corrected."
      ],
      "metadata": {
        "id": "RBvYa9QsVW3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "# Only fix row with \"RESIDENTIAL BUILDING\" in \"Location Type\" column\n",
        "df = df.withColumn('Location Type', regexp_replace('Location Type', \n",
        "                                              'RESIDENTIAL BUILDING', \n",
        "                                              'Residential Building'))"
      ],
      "metadata": {
        "id": "KdlIb6XSVYKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try some columns to see the improvement!\n",
        "\n",
        "Below are a few cells that show some of the columns that had uniformity problems. We can observe by running these cells that the items of each column now contain uniform casing (i.e. every value is in the format where the first letter of each word is uppercased).\n",
        "\n",
        "(The output is currently hidden, press the show output button if you want to see our results)"
      ],
      "metadata": {
        "id": "Ru6mR7EqVaRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# View column \"Complaint Type\" \n",
        "df.select(\"Complaint Type\").distinct().collect()"
      ],
      "metadata": {
        "id": "On_bX_9IVcs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# View column \"Location Type\"\n",
        "df.select(\"Location Type\").distinct().collect()"
      ],
      "metadata": {
        "id": "NsClBPQlVfgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pyspark\n",
        "# View column \"City\"\n",
        "df.select(\"City\").distinct().collect()"
      ],
      "metadata": {
        "id": "ozudOHJFVf8g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}